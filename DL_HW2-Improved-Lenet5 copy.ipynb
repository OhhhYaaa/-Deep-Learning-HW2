{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1c2a45-718a-4ccf-8a22-e6e6d4959cc8",
   "metadata": {},
   "source": [
    "# DL_HW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f85e11-365b-4241-b517-a6db3aa19bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os, pickle, json, random\n",
    "\n",
    "from imageio import imread, imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from skimage import feature, data, color\n",
    "import skimage\n",
    "import cv2\n",
    "from functools import partial\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import decimal\n",
    "\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(cpus)\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00099a9-58d5-4ccc-9117-e5ca5e3e6770",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tool Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38d204e-dd07-4af3-ad8e-5cc9dc9788ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot\n",
    "def self_onehot(x, c = 50) : \n",
    "    x_onehot = np.zeros([x.shape[0], c])\n",
    "    for i in range(x.shape[0]) :\n",
    "        x_onehot[i, int(x[i])] = 1\n",
    "    return x_onehot\n",
    "\n",
    "# 讀取圖片function\n",
    "def read_img(path) :\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (256, 256))\n",
    "    return img\n",
    "\n",
    "# color histogram\n",
    "def color_histogram(img, hist_size = 256):\n",
    "    img = img.astype(np.uint8)\n",
    "    color = ('b','g','r')\n",
    "    histogram = []\n",
    "    for idx, color in enumerate(color):\n",
    "        h = cv2.calcHist(img,[idx],None,[hist_size],[0, 256])\n",
    "        histogram.append(h)\n",
    "    return np.array(histogram).squeeze().reshape(1, -1)\n",
    "# histogram of gradient\n",
    "def get_hog_feature(img) :\n",
    "    img = img.astype(np.uint8)\n",
    "    hog_vec, hog_vis = feature.hog(img, \n",
    "                               pixels_per_cell=(64, 64), \n",
    "                               cells_per_block = [1, 1],\n",
    "                               channel_axis = 2, \n",
    "                               visualize=True)\n",
    "    return hog_vec\n",
    "\n",
    "# haralick\n",
    "def get_haralick(img): \n",
    "    values_temp = []\n",
    "    img = img.astype(np.uint8)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    glcm = skimage.feature.graycomatrix(img, [2, 8, 16], [0, np.pi / 4, np.pi / 2, np.pi * 3 / 4], 256, symmetric=True, normed=True)  # , np.pi / 4, np.pi / 2, np.pi * 3 / 4\n",
    "    # print(glcm.shape) \n",
    "    for prop in {'contrast', 'dissimilarity','homogeneity', 'energy', 'correlation', 'ASM'}:\n",
    "        temp = skimage.feature.graycoprops(glcm, prop)\n",
    "        temp = np.array(temp).reshape(-1)\n",
    "        values_temp.append(temp)\n",
    "        # print(prop, temp)\n",
    "        # print('len:',len(temp))\n",
    "        # print('')\n",
    "    values_temp = np.array(values_temp).reshape(1, -1)\n",
    "    return (values_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07b363-6f3a-4754-9baa-98d30a8faf28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read Data and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1c241-df86-4939-9165-277a334cf521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取index\n",
    "os.chdir('/home/rita/111/111-2DL/HW1')\n",
    "train_idx = pd.read_table('train.txt', header = None, sep = ' ')\n",
    "val_idx = pd.read_table('val.txt', header = None, sep = ' ')\n",
    "test_idx = pd.read_table('test.txt', header = None, delimiter = ' ')\n",
    "train_idx = np.array(train_idx)\n",
    "val_idx = np.array(val_idx)\n",
    "test_idx = np.array(test_idx)\n",
    "train_y = train_idx[::, 1].astype(float)\n",
    "val_y = val_idx[::, 1].astype(float)\n",
    "test_y = test_idx[::, 1].astype(float)\n",
    "os.chdir('/home/rita/111/111-2DL/HW2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae968c-ec8d-4d67-bab2-252fd98aa2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_onehot = self_onehot(train_y)\n",
    "val_y_onehot = self_onehot(val_y)\n",
    "test_y_onehot = self_onehot(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303ccc5-6121-4193-98e5-72c8b19e11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pic = np.load('./data/train_pic.npy')\n",
    "val_pic = np.load('./data/val_pic.npy')\n",
    "test_pic = np.load('./data/test_pic.npy')\n",
    "train_x = np.load('./data/train_x.npy')\n",
    "val_x = np.load('./data/val_x.npy')\n",
    "test_x = np.load('./data/test_x.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取圖片function\n",
    "def read_img_32(path) :\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "os.chdir('/home/rita/111/111-2DL/HW1')\n",
    "if __name__ == '__main__' : \n",
    "    with Pool(processes = 80) as p:\n",
    "        train_pic_32 = list(tqdm(p.imap(read_img_32, train_idx[::, 0], chunksize=100), total = train_idx.shape[0]))\n",
    "        val_pic_32 = list(tqdm(p.imap(read_img_32, val_idx[::, 0], chunksize=100), total = val_idx.shape[0]))\n",
    "        test_pic_32 = list(tqdm(p.imap(read_img_32, test_idx[::, 0], chunksize=100), total = test_idx.shape[0]))\n",
    "os.chdir('/home/rita/111/111-2DL/HW2')\n",
    "\n",
    "train_pic_32 = np.array(train_pic_32)\n",
    "val_pic_32 = np.array(val_pic_32)\n",
    "test_pic_32 = np.array(test_pic_32)\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = train_pic_32, train_y_onehot, test_pic_32, test_y_onehot\n",
    "X_val, Y_val = val_pic_32, val_y_onehot\n",
    "X_train, X_val, X_test = X_train/float(255), X_val/float(255), X_test/float(255)\n",
    "X_train -= np.mean(X_train)\n",
    "X_val -= np.mean(X_val)\n",
    "X_test -= np.mean(X_test)\n",
    "X_train = np.transpose(X_train, (0, 3, 1, 2))\n",
    "X_val = np.transpose(X_val, (0, 3, 1, 2))\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
    "\n",
    "# train_dataloader = Self_DataLoader(X_train, train_y_onehot, batch_size = 32, shuffle=True)\n",
    "# val_dataloader = Self_DataLoader(X_val, val_y_onehot, batch_size = 32, shuffle=True)\n",
    "# test_dataloader = Self_DataLoader(X_test, test_y_onehot, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aee710-c833-40e7-a420-2a454acd04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_DataLoader() :\n",
    "    def __init__(self, data, label, batch_size = 64, shuffle = False) :\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.n_sample = data.shape[0]\n",
    "        self.n_batches = (self.n_sample // self.batch_size) # + 1\n",
    "        self.start = 0\n",
    "        self.end = self.start + self.batch_size\n",
    "        if self.shuffle : np.random.shuffle(np.arange(self.n_sample))\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __next__(self):\n",
    "        if self.end >= self.n_sample :\n",
    "            self.start = 0\n",
    "            self.end = self.start + self.batch_size\n",
    "            if self.shuffle : np.random.shuffle(np.arange(self.n_sample))\n",
    "            raise StopIteration\n",
    "        datas = self.data[self.start : self.end]\n",
    "        labels = self.label[self.start : self.end]\n",
    "        \n",
    "        self.start += self.batch_size\n",
    "        self.end += self.batch_size\n",
    "        return datas, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42451054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 曾\n",
    "class Self_DataLoader():\n",
    "    def __init__(self, data, labels, batch_size=32, shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = data.shape[0]\n",
    "        self.num_batches = int(np.ceil(self.num_samples / self.batch_size))\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            self.current_batch = 0\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.indices)\n",
    "            raise StopIteration\n",
    "            \n",
    "        batch_indices = self.indices[self.current_batch*self.batch_size : (self.current_batch+1)*self.batch_size]\n",
    "        batch_data = self.data[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        \n",
    "        self.current_batch += 1\n",
    "        \n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8460d-11d9-4705-9c15-8bfc7b513aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reference : https://github.com/toxtli/lenet-5-mnist-from-scratch-numpy/blob/master/app.py\n",
    "\n",
    "class FC():\n",
    "    def __init__(self, D_in, D_out):\n",
    "        self.cache = None\n",
    "        self.W = {'val': np.random.normal(0.0, np.sqrt(2/D_in), (D_in,D_out)), 'grad': 0}\n",
    "        self.b = {'val': np.random.randn(D_out), 'grad': 0}\n",
    "\n",
    "    def _forward(self, X):\n",
    "        out = np.dot(X, self.W['val']) + self.b['val']\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = np.dot(dout, self.W['val'].T).reshape(X.shape)\n",
    "        self.W['grad'] = np.dot(X.reshape(X.shape[0], np.prod(X.shape[1:])).T, dout)\n",
    "        self.b['grad'] = np.sum(dout, axis=0)\n",
    "        #self._update_params()\n",
    "        return dX\n",
    "\n",
    "    def _update_params(self, lr=0.001):\n",
    "        # Update the parameters\n",
    "        self.W['val'] -= lr*self.W['grad']\n",
    "        self.b['val'] -= lr*self.b['grad']\n",
    "\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        out = np.maximum(0, X)\n",
    "        self.cache = X\n",
    "        return out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = np.array(dout, copy=True)\n",
    "        dX[X <= 0] = 0\n",
    "        return dX\n",
    "\n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    Softmax activation layer\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        #print(\"Build Softmax\")\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        #print(\"Softmax: _forward\")\n",
    "        maxes = np.amax(X, axis=1)\n",
    "        maxes = maxes.reshape(maxes.shape[0], 1)\n",
    "        Y = np.exp(X - maxes)\n",
    "        Z = Y / np.sum(Y, axis=1).reshape(Y.shape[0], 1)\n",
    "        self.cache = (X, Y, Z)\n",
    "        return Z # distribution\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X, Y, Z = self.cache\n",
    "        dZ = np.zeros(X.shape)\n",
    "        dY = np.zeros(X.shape)\n",
    "        dX = np.zeros(X.shape)\n",
    "        N = X.shape[0]\n",
    "        for n in range(N):\n",
    "            i = np.argmax(Z[n])\n",
    "            dZ[n,:] = np.diag(Z[n]) - np.outer(Z[n],Z[n])\n",
    "            M = np.zeros((N,N))\n",
    "            M[:,i] = 1\n",
    "            dY[n,:] = np.eye(N) - M\n",
    "        dX = np.dot(dout,dZ)\n",
    "        dX = np.dot(dX,dY)\n",
    "        return dX\n",
    "    \n",
    "    \n",
    "class Conv():\n",
    "    def __init__(self, Cin, Cout, F, stride=1, padding=0, bias=True):\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.W = {'val': np.random.normal(0.0,np.sqrt(2/Cin),(Cout,Cin,F,F)), 'grad': 0} # Xavier Initialization\n",
    "        self.b = {'val': np.random.randn(Cout), 'grad': 0}\n",
    "        self.cache = None\n",
    "        self.pad = padding\n",
    "\n",
    "    def _forward(self, X):\n",
    "        X = np.pad(X, ((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)), 'constant')\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        Y = np.zeros((N, self.Cout, H_, W_))\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(self.Cout):\n",
    "                for h in range(H_):\n",
    "                    for w in range(W_):\n",
    "                        Y[n, c, h, w] = np.sum(X[n, :, h:h+self.F, w:w+self.F] * self.W['val'][c, :, :, :]) + self.b['val'][c]\n",
    "\n",
    "        self.cache = X\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        # dout (N,Cout,H_,W_)\n",
    "        # W (Cout, Cin, F, F)\n",
    "        X = self.cache\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        W_rot = np.rot90(np.rot90(self.W['val']))\n",
    "\n",
    "        dX = np.zeros(X.shape)\n",
    "        dW = np.zeros(self.W['val'].shape)\n",
    "        db = np.zeros(self.b['val'].shape)\n",
    "\n",
    "        # dW\n",
    "        for co in range(self.Cout):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(self.F):\n",
    "                    for w in range(self.F):\n",
    "                        dW[co, ci, h, w] = np.sum(X[:,ci,h:h+H_,w:w+W_] * dout[:,co,:,:])\n",
    "\n",
    "        # db\n",
    "        for co in range(self.Cout):\n",
    "            db[co] = np.sum(dout[:,co,:,:])\n",
    "\n",
    "        dout_pad = np.pad(dout, ((0,0),(0,0),(self.F,self.F),(self.F,self.F)), 'constant')\n",
    "        #print(\"dout_pad.shape: \" + str(dout_pad.shape))\n",
    "        # dX\n",
    "        for n in range(N):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(H):\n",
    "                    for w in range(W):\n",
    "                        #print(\"self.F.shape: %s\", self.F)\n",
    "                        #print(\"%s, W_rot[:,ci,:,:].shape: %s, dout_pad[n,:,h:h+self.F,w:w+self.F].shape: %s\" % ((n,ci,h,w),W_rot[:,ci,:,:].shape, dout_pad[n,:,h:h+self.F,w:w+self.F].shape))\n",
    "                        dX[n, ci, h, w] = np.sum(W_rot[:,ci,:,:] * dout_pad[n, :, h:h+self.F,w:w+self.F])\n",
    "\n",
    "        return dX\n",
    "\n",
    "class MaxPool():\n",
    "    def __init__(self, F, stride):\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # X: (N, Cin, H, W): maxpool along 3rd, 4th dim\n",
    "        (N,Cin,H,W) = X.shape\n",
    "        F = self.F\n",
    "        W_ = int(float(W)/F)\n",
    "        H_ = int(float(H)/F)\n",
    "        Y = np.zeros((N,Cin,W_,H_))\n",
    "        M = np.zeros(X.shape) # mask\n",
    "        for n in range(N):\n",
    "            for cin in range(Cin):\n",
    "                for w_ in range(W_):\n",
    "                    for h_ in range(H_):\n",
    "                        Y[n,cin,w_,h_] = np.max(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)])\n",
    "                        i,j = np.unravel_index(X[n,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)].argmax(), (F,F))\n",
    "                        M[n,cin,F*w_+i,F*h_+j] = 1\n",
    "        self.cache = M\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        M = self.cache\n",
    "        (N,Cin,H,W) = M.shape\n",
    "        dout = np.array(dout)\n",
    "        #print(\"dout.shape: %s, M.shape: %s\" % (dout.shape, M.shape))\n",
    "        dX = np.zeros(M.shape)\n",
    "        for n in range(N):\n",
    "            for c in range(Cin):\n",
    "                #print(\"(n,c): (%s,%s)\" % (n,c))\n",
    "                dX[n,c,:,:] = dout[n,c,:,:].repeat(2, axis=0).repeat(2, axis=1)\n",
    "        return dX*M\n",
    "\n",
    "def NLLLoss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    N = Y_pred.shape[0]\n",
    "    M = np.sum(Y_pred*Y_true, axis=1)\n",
    "    for e in M:\n",
    "        #print(e)\n",
    "        if e == 0:\n",
    "            loss += 500\n",
    "        else:\n",
    "            loss += -np.log(e)\n",
    "    return loss/N\n",
    "    \n",
    "class CrossEntropyLoss():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get(self, Y_pred, Y_true):\n",
    "        N = Y_pred.shape[0]\n",
    "        softmax = Softmax()\n",
    "        prob = softmax._forward(Y_pred)\n",
    "        loss = NLLLoss(prob, Y_true)\n",
    "        Y_serial = np.argmax(Y_true, axis=1)\n",
    "        dout = prob.copy()\n",
    "        dout[np.arange(N), Y_serial] -= 1\n",
    "        return loss, dout\n",
    "\n",
    "class Net(metaclass=ABCMeta):\n",
    "    # Neural network super class\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, dout):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_params(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_params(self, params):\n",
    "        pass\n",
    "\n",
    "class SGD():\n",
    "    def __init__(self, params, lr=0.001, reg=0):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.parameters:\n",
    "            param['val'] -= (self.lr*param['grad'] + self.reg*param['val'])\n",
    "    \n",
    "class SGDMomentum():\n",
    "    def __init__(self, params, lr=0.001, momentum=0.99, reg=0):\n",
    "        self.l = len(params)\n",
    "        self.parameters = params\n",
    "        self.velocities = []\n",
    "        for param in self.parameters:\n",
    "            self.velocities.append(np.zeros(param['val'].shape))\n",
    "        self.lr = lr\n",
    "        self.rho = momentum\n",
    "        self.reg = reg\n",
    "\n",
    "    def step(self):\n",
    "        for i in range(self.l):\n",
    "            self.velocities[i] = self.rho*self.velocities[i] + (1-self.rho)*self.parameters[i]['grad']\n",
    "            self.parameters[i]['val'] -= (self.lr*self.velocities[i] + self.reg*self.parameters[i]['val'])\n",
    "\n",
    "def softmax(self, x) :\n",
    "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims = True)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baa2a36",
   "metadata": {},
   "source": [
    "### Improved LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f614272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Conv():\n",
    "    def __init__(self, Cin, Cout, F, stride=1, padding=0, bias=True):\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.W = {'val': np.random.normal(0.0,np.sqrt(2/Cin),(Cout,Cin,F,F)), 'grad': 0} # Xavier Initialization\n",
    "        self.b = {'val': np.random.randn(Cout), 'grad': 0}\n",
    "        self.cache = None\n",
    "        self.pad = padding\n",
    "\n",
    "    def _forward(self, X):\n",
    "        X = np.pad(X, ((0,0),(0,0),(self.pad,self.pad),(self.pad,self.pad)), 'constant')\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        Y = np.zeros((N, self.Cout, H_, W_))\n",
    "\n",
    "        # for n in range(N):\n",
    "        for c in range(self.Cout):\n",
    "            for h in range(H_):\n",
    "                for w in range(W_):\n",
    "                    Y[:, c, h, w] = np.sum(X[:, :, h:h+self.F, w:w+self.F] * self.W['val'][c, :, :, :], axis = (1, 2, 3)) + self.b['val'][c]\n",
    "\n",
    "        self.cache = X\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        # dout (N,Cout,H_,W_)\n",
    "        # W (Cout, Cin, F, F)\n",
    "        X = self.cache\n",
    "        (N, Cin, H, W) = X.shape\n",
    "        H_ = H - self.F + 1\n",
    "        W_ = W - self.F + 1\n",
    "        W_rot = np.rot90(np.rot90(self.W['val']))\n",
    "\n",
    "        dX = np.zeros(X.shape)\n",
    "        dW = np.zeros(self.W['val'].shape)\n",
    "        db = np.zeros(self.b['val'].shape)\n",
    "\n",
    "        # dW\n",
    "        for co in range(self.Cout):\n",
    "            for ci in range(Cin):\n",
    "                for h in range(self.F):\n",
    "                    for w in range(self.F):\n",
    "                        dW[co, ci, h, w] = np.sum(X[:,ci,h:h+H_,w:w+W_] * dout[:,co,:,:])\n",
    "\n",
    "        # db\n",
    "        for co in range(self.Cout):\n",
    "            db[co] = np.sum(dout[:,co,:,:])\n",
    "\n",
    "        dout_pad = np.pad(dout, ((0,0),(0,0),(self.F,self.F),(self.F,self.F)), 'constant')\n",
    "        #print(\"dout_pad.shape: \" + str(dout_pad.shape))\n",
    "        # dX\n",
    "        # for n in range(N):\n",
    "        for ci in range(Cin):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    #print(\"self.F.shape: %s\", self.F)\n",
    "                    #print(\"%s, W_rot[:,ci,:,:].shape: %s, dout_pad[n,:,h:h+self.F,w:w+self.F].shape: %s\" % ((n,ci,h,w),W_rot[:,ci,:,:].shape, dout_pad[n,:,h:h+self.F,w:w+self.F].shape))\n",
    "                    dX[:, ci, h, w] = np.sum(W_rot[:,ci,:,:] * dout_pad[:, :, h:h+self.F,w:w+self.F], axis = (1, 2, 3))\n",
    "\n",
    "        return dX\n",
    "    \n",
    "class Self_MaxPool():\n",
    "    def __init__(self, F, stride):\n",
    "        self.F = F\n",
    "        self.S = stride\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        # X: (N, Cin, H, W): maxpool along 3rd, 4th dim\n",
    "        (N,Cin,H,W) = X.shape\n",
    "        F = self.F\n",
    "        W_ = int(float(W)/F)\n",
    "        H_ = int(float(H)/F)\n",
    "        Y = np.zeros((N,Cin,W_,H_))\n",
    "        M = np.zeros(X.shape) # mask\n",
    "            \n",
    "        for cin in range(Cin):\n",
    "            for w_ in range(W_):\n",
    "                for h_ in range(H_):\n",
    "                    Y[:,cin,w_,h_] = np.max(X[:,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)], axis=(1, 2))\n",
    "                    i,j = np.unravel_index(X[:,cin,F*w_:F*(w_+1),F*h_:F*(h_+1)].reshape(N, -1).argmax(axis=1), (F,F))\n",
    "                    M[:,cin,F*w_+i,F*h_+j] = 1\n",
    "        self.cache = M\n",
    "        return Y\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        M = self.cache\n",
    "        (N,Cin,H,W) = M.shape\n",
    "        dout = np.array(dout)\n",
    "        #print(\"dout.shape: %s, M.shape: %s\" % (dout.shape, M.shape))\n",
    "        dX = np.zeros(M.shape)\n",
    "        for c in range(Cin):\n",
    "            #print(\"(n,c): (%s,%s)\" % (n,c))\n",
    "            dX[:,c,:,:] = dout[:,c,:,:].repeat(2, axis=1).repeat(2, axis=2)\n",
    "        return dX*M\n",
    "\n",
    "# 自己改的  \n",
    "class Self_Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        x = np.float128(X)\n",
    "        self.cache = 1 / (1 + np.exp(-x))\n",
    "        return np.float32(self.cache)\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        SX = self.cache\n",
    "        dX = dout*SX*(1-SX)\n",
    "        return dX\n",
    "    \n",
    "# 題目要求\n",
    "class Improved_Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        self.x = None\n",
    "\n",
    "    def _forward(self, X):\n",
    "        self.x = np.float128(X)\n",
    "        # X = np.float128(X)\n",
    "        self.cache = self.x / (1 + np.exp(-self.x))\n",
    "\n",
    "        return self.cache\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        # self.x = np.float128(self.x)\n",
    "        sx1 = 1 / (1 + np.exp(-self.x))\n",
    "        sx2 = 1 / (1 + np.exp(self.x))\n",
    "        dX = dout * (sx1 + self.cache * sx2)\n",
    "        \n",
    "        return dX\n",
    "    \n",
    "\n",
    "# zhen \n",
    "class Improved_Sigmoid():\n",
    "    \"\"\"\n",
    "    ImprovedSigmoid(x) = x * Sigmoid(x)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.cache = None\n",
    "        self.out = None\n",
    "        self.sigmoid = Self_Sigmoid()\n",
    "\n",
    "    def _forward(self, X):\n",
    "        X = np.float128(X)\n",
    "        self.cache = X\n",
    "        self.out = X * self.sigmoid._forward(X)\n",
    "        return self.out\n",
    "\n",
    "    def _backward(self, dout):\n",
    "        X = self.cache\n",
    "        dX = dout*(self.sigmoid._forward(X) + self.out * self.sigmoid._forward(-X))\n",
    "        return dX\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce57445-d24d-4c8c-b268-3e1bede02550",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Improved_LeNet5(Net):\n",
    "    # LeNet5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv1 = Self_Conv(3, 6, 3) # C1(3*32*32 --> 6*30*30)\n",
    "        self.Sigmoid1 = Improved_Sigmoid()\n",
    "        self.pool1 = Self_MaxPool(2,2) # S2(6*30*30  --> 6*15*15)\n",
    "        self.conv2 = Self_Conv(6, 16, 3) # C3(6*15*15 --> 16*13*13)\n",
    "        self.Sigmoid2 = Improved_Sigmoid()\n",
    "        # 16*13*13 --> 16*14*14\n",
    "        self.pool2 = Self_MaxPool(2,2) # S4(16*14*14 --> 16*7*7)\n",
    "        # 16*7*7 -> 16*8*8\n",
    "        self.conv3 = Self_Conv(16, 6, 3) # C3(16*8*8 --> 6*6*6) (16*7*7 --> 6*5*5)\n",
    "        self.Sigmoid3 = Improved_Sigmoid()\n",
    "        # self.pool3 = MaxPool(2,2) # S4(6*6*6 --> 6*3*3)\n",
    "        \n",
    "        self.FC1 = FC(6*6*6, 120) # C5()\n",
    "        # self.FC1 = FC(6*6*6, 120) # C5()\n",
    "\n",
    "        self.Sigmoid4 = Improved_Sigmoid()\n",
    "        self.FC2 = FC(120, 84) # F6\n",
    "        self.Sigmoid5 = Improved_Sigmoid()\n",
    "        self.FC3 = FC(84, 50)\n",
    "        self.Softmax = Softmax()\n",
    "\n",
    "        self.p3_shape = None\n",
    "        self.a3_shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1._forward(X)\n",
    "        a1 = self.Sigmoid1._forward(h1)\n",
    "        p1 = self.pool1._forward(a1)\n",
    "        h2 = self.conv2._forward(p1)\n",
    "        a2 = self.Sigmoid2._forward(h2)\n",
    "        a2 = np.pad(a2, ((0, 0), (0, 0), (0, 1), (0, 1)), 'constant')\n",
    "        p2 = self.pool2._forward(a2)\n",
    "        p2 = np.pad(p2, ((0, 0), (0, 0), (0, 1), (0, 1)), 'constant')\n",
    "        \n",
    "        h3 = self.conv3._forward(p2)\n",
    "        a3 = self.Sigmoid3._forward(h3)\n",
    "        # p3 = self.pool3._forward(a2)\n",
    "        self.a3_shape = a3.shape\n",
    "        fl = a3.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1._forward(fl)\n",
    "        a3 = self.Sigmoid4._forward(h3)\n",
    "        h4 = self.FC2._forward(a3)\n",
    "        a5 = self.Sigmoid5._forward(h4)\n",
    "        h5 = self.FC3._forward(a5)\n",
    "        a5 = self.Softmax._forward(h5)\n",
    "        return a5\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout = self.Softmax._backward(dout)\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.Sigmoid5._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.Sigmoid4._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "        dout = dout.reshape(self.a3_shape) # reshape\n",
    "        # dout = self.pool3._backward(dout)\n",
    "        dout = self.Sigmoid3._backward(dout)\n",
    "        dout = self.conv3._backward(dout)\n",
    "        dout = dout[::, ::, :7, :7]\n",
    "        dout = self.pool2._backward(dout)\n",
    "        dout = dout[::, ::, :13, :13]\n",
    "        dout = self.Sigmoid2._backward(dout)\n",
    "        dout = self.conv2._backward(dout)\n",
    "        dout = self.pool1._backward(dout)\n",
    "        dout = self.Sigmoid1._backward(dout)\n",
    "        dout = self.conv1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.conv3.W, self.conv3.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.conv3.W, self.conv3.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996b076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Improved_LeNet5_21(Net):\n",
    "    # LeNet5\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conv1 = Self_Conv(3, 6, 3) # C1(3*32*32 --> 6*30*30)\n",
    "        self.Sigmoid1 = Improved_Sigmoid()\n",
    "        # self.pool1 = Self_MaxPool(2,2) # S2(6*30*30  --> 6*15*15)\n",
    "        self.conv2 = Self_Conv(6, 16, 3) # C3(6*15*15 --> 16*13*13)\n",
    "        self.Sigmoid2 = Improved_Sigmoid()\n",
    "        # 16*13*13 --> 16*14*14\n",
    "        self.pool2 = Self_MaxPool(2,2) # S4(16*14*14 --> 16*7*7)\n",
    "        # 16*7*7 -> 16*8*8\n",
    "        self.conv3 = Self_Conv(16, 6, 3) # C3(16*8*8 --> 6*6*6) (16*7*7 --> 6*5*5)\n",
    "        self.Sigmoid3 = Improved_Sigmoid()\n",
    "        self.pool3 = MaxPool(2,2) # S4(6*6*6 --> 6*3*3)\n",
    "        \n",
    "        self.FC1 = FC(6*6*6, 120) # C5()\n",
    "        # self.FC1 = FC(6*6*6, 120) # C5()\n",
    "\n",
    "        self.Sigmoid4 = Improved_Sigmoid()\n",
    "        self.FC2 = FC(120, 84) # F6\n",
    "        self.Sigmoid5 = Improved_Sigmoid()\n",
    "        self.FC3 = FC(84, 50)\n",
    "        self.Softmax = Softmax()\n",
    "\n",
    "        self.p3_shape = None\n",
    "        self.a3_shape = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        h1 = self.conv1._forward(X)\n",
    "        a1 = self.Sigmoid1._forward(h1)\n",
    "        # p1 = self.pool1._forward(a1)\n",
    "        h2 = self.conv2._forward(a1)\n",
    "        a2 = self.Sigmoid2._forward(h2)\n",
    "        # a2 = np.pad(a2, ((0, 0), (0, 0), (0, 1), (0, 1)), 'constant')\n",
    "        p2 = self.pool2._forward(a2)\n",
    "        # p2 = np.pad(p2, ((0, 0), (0, 0), (0, 1), (0, 1)), 'constant')\n",
    "        \n",
    "        h3 = self.conv3._forward(p2)\n",
    "        a3 = self.Sigmoid3._forward(h3)\n",
    "        p3 = self.pool3._forward(a2)\n",
    "        self.p3_shape = p3.shape\n",
    "        # self.a3_shape = a3.shape\n",
    "        fl = a3.reshape(X.shape[0],-1) # Flatten\n",
    "        h3 = self.FC1._forward(fl)\n",
    "        a3 = self.Sigmoid4._forward(h3)\n",
    "        h4 = self.FC2._forward(a3)\n",
    "        a5 = self.Sigmoid5._forward(h4)\n",
    "        h5 = self.FC3._forward(a5)\n",
    "        a5 = self.Softmax._forward(h5)\n",
    "        return a5\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout = self.Softmax._backward(dout)\n",
    "        dout = self.FC3._backward(dout)\n",
    "        dout = self.Sigmoid5._backward(dout)\n",
    "        dout = self.FC2._backward(dout)\n",
    "        dout = self.Sigmoid4._backward(dout)\n",
    "        dout = self.FC1._backward(dout)\n",
    "        # dout = dout.reshape(self.a3_shape) # reshape\n",
    "        dout = dout.reshape(self.p3_shape) # reshape\n",
    "        dout = self.pool3._backward(dout)\n",
    "        dout = self.Sigmoid3._backward(dout)\n",
    "        dout = self.conv3._backward(dout)\n",
    "        # dout = dout[::, ::, :7, :7]\n",
    "        dout = self.pool2._backward(dout)\n",
    "        # dout = dout[::, ::, :13, :13]\n",
    "        dout = self.Sigmoid2._backward(dout)\n",
    "        dout = self.conv2._backward(dout)\n",
    "        # dout = self.pool1._backward(dout)\n",
    "        dout = self.Sigmoid1._backward(dout)\n",
    "        dout = self.conv1._backward(dout)\n",
    "\n",
    "    def get_params(self):\n",
    "        return [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.conv3.W, self.conv3.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        [self.conv1.W, self.conv1.b, self.conv2.W, self.conv2.b, self.conv3.W, self.conv3.b, self.FC1.W, self.FC1.b, self.FC2.W, self.FC2.b, self.FC3.W, self.FC3.b] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1baf9-33fb-47d6-8126-b579a606a733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Self_DataLoader(X_train, train_y, batch_size = 128, shuffle=True)\n",
    "\n",
    "model = Improved_LeNet5_21()\n",
    "optim = SGD(model.get_params(), lr=1e-3, reg=0)\n",
    "criterion = CrossEntropyLoss()\n",
    "print('Start Training !')\n",
    "# TRAIN\n",
    "# 加 dataloader 版本\n",
    "n_epochs = 30\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for i in range(n_epochs):\n",
    "    # train\n",
    "    temp_train_loss, temp_train_acc = 0, 0\n",
    "    with tqdm(total=train_dataloader.num_batches) as pbar:\n",
    "        for X_batch, Y_batch in train_dataloader :\n",
    "            Y_batch = self_onehot(Y_batch)\n",
    "            Y_pred = model.forward(X_batch)\n",
    "            loss, _ = criterion.get(Y_pred, Y_batch)\n",
    "            dout = Y_pred - Y_batch  # pred - label\n",
    "            model.backward(dout)\n",
    "            optim.step()\n",
    "            \n",
    "            pred_y = np.argmax(Y_pred, axis=1)\n",
    "            acc = np.mean(pred_y == np.argmax(Y_batch, axis=1))\n",
    "            \n",
    "            temp_train_loss += loss\n",
    "            temp_train_acc += acc\n",
    "             \n",
    "            pbar.update(1)\n",
    "    temp_train_loss /= train_dataloader.num_batches   \n",
    "    temp_train_acc /= train_dataloader.num_batches  \n",
    "    train_acc.append(temp_train_acc)\n",
    "    train_loss.append(temp_train_loss)\n",
    "    \n",
    "    print(\"%s%% Epoch: %s, loss: %.5f\" % (100*i/n_epochs, i + 1, loss))\n",
    "    \n",
    "    # validation\n",
    "    Y_pred = model.forward(X_val)\n",
    "    pred_y = np.argmax(Y_pred, axis=1)\n",
    "    acc = np.mean(pred_y == val_y)\n",
    "    # Y_batch = self_onehot(Y_val)\n",
    "    loss, _ = criterion.get(Y_pred, Y_val)\n",
    "    val_loss.append(loss)\n",
    "    val_acc.append(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc64631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save params\n",
    "weights = model.get_params()\n",
    "with open(\"./model/Improved_lenet5.pkl\",\"wb\") as f:\n",
    "    pickle.dump(weights, f)\n",
    "\n",
    "with open(\"./loss/Improved_train_loss.txt\", \"w\") as fp:\n",
    "    json.dump(np.array(train_loss, dtype=np.float64).tolist(), fp)\n",
    "with open(\"./acc/Improved_train_acc.txt\", \"w\") as fp:\n",
    "    json.dump(train_acc, fp)    \n",
    "with open(\"./loss/Improved_val_loss.txt\", \"w\") as fp:\n",
    "    json.dump(np.array(val_loss, dtype=np.float64).tolist(), fp)   \n",
    "with open(\"./acc/Improved_val_acc.txt\", \"w\") as fp:\n",
    "    json.dump(val_acc, fp)    \n",
    "    \n",
    "# draw\n",
    "plt.title('Improved_Loss') \n",
    "plt.plot(range(n_epochs), train_loss, label=\"train\")\n",
    "plt.plot(range(n_epochs), val_loss, label=\"val\", c = 'red')\n",
    "plt.legend()\n",
    "plt.savefig('./figure/Improved_lenet5_loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Improved_Accuracy')\n",
    "plt.plot(range(n_epochs), train_acc, label=\"train\")\n",
    "plt.plot(range(n_epochs), val_acc, label=\"val\", c = 'red')\n",
    "plt.legend()\n",
    "plt.savefig('./figure/Improved_lenet5_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./loss/Improved_train_loss.txt\", \"r\") as fp:\n",
    "    train_loss = json.load(fp)\n",
    "with open(\"./loss/Improved_val_loss.txt\", \"r\") as fp:\n",
    "    val_loss = json.load(fp)\n",
    "with open(\"./acc/Improved_train_acc.txt\", \"r\") as fp:\n",
    "    train_acc = json.load(fp)\n",
    "with open(\"./acc/Improved_val_acc.txt\", \"r\") as fp:\n",
    "    val_acc = json.load(fp)\n",
    "    \n",
    "# draw\n",
    "plt.title('Improved_Loss')\n",
    "plt.plot(range(n_epochs), train_loss, label=\"train\")\n",
    "plt.plot(range(n_epochs), val_loss, label=\"val\", c = 'red')\n",
    "plt.legend()\n",
    "plt.savefig('./figure/Improved_lenet5_loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Improved_Accuracy')\n",
    "plt.plot(range(n_epochs), train_acc, label=\"train\")\n",
    "plt.plot(range(n_epochs), val_acc, label=\"val\", c = 'red')\n",
    "plt.legend()\n",
    "plt.savefig('./figure/Improved_lenet5_acc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/Improved_lenet5.pkl\",\"rb\") as fp:\n",
    "    weight = pickle.load(fp)\n",
    "    \n",
    "loaded_model = Improved_LeNet5()\n",
    "loaded_model.set_params(weight)\n",
    "optim = SGD(loaded_model.get_params(), lr=0.0001, reg=0)\n",
    "criterion = CrossEntropyLoss()\n",
    "s = time.time()\n",
    "\n",
    "Y_pred = model.forward(X_val)\n",
    "pred_y = np.argmax(Y_pred, axis=1)\n",
    "acc = np.mean(pred_y == val_y)\n",
    "print('Improved Val Accuracy : ', acc)\n",
    "\n",
    "Y_pred = model.forward(X_test)\n",
    "pred_y = np.argmax(Y_pred, axis=1)\n",
    "acc = np.mean(pred_y == test_y)\n",
    "print('Improved Test Accuracy : ', acc)\n",
    "\n",
    "\n",
    "print(time.time() - s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
